{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fec24e0",
   "metadata": {},
   "source": [
    "# Training Unet model on synthetic data and testing it on AI4Mars dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21618f0c",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9280cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA version: 12.8\n",
      "Torch version: 2.9.0+cu128\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = r\"/home/uname/Unity-Mars-Simulation/My project/Assets/TerrainLayerImages/Images\"\n",
    "MASK_DIR = r\"/home/uname/Unity-Mars-Simulation/My project/Assets/TerrainLayerImages/Labels\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb9061",
   "metadata": {},
   "source": [
    "## The colour maping is different in synthetic and real labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43448b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define color mappings ---\n",
    "\n",
    "COLOR_MAP = {\n",
    "    (255,   0,   0): 0,  # red - soil\n",
    "    (0,   255,   0): 1,  # green - bedrock\n",
    "    (255, 235,   4): 2,  # yellow (actual Unity export)\n",
    "    (128, 128, 128): 3,  # gray - bigrock\n",
    "    (0,     0, 255): 4,  # blue - sky\n",
    "}\n",
    "\n",
    "\n",
    "# For testing masks (0,0,0), (1,1,1), (2,2,2), (3,3,3), (255,255,255)\n",
    "ALT_COLOR_MAP = {\n",
    "    (0, 0, 0): 0,\n",
    "    (1, 1, 1): 1,\n",
    "    (2, 2, 2): 2,\n",
    "    (3, 3, 3): 3,\n",
    "    (255, 255, 255): 4\n",
    "}\n",
    "\n",
    "\n",
    "def rgb_to_class(mask, color_map):\n",
    "    \"\"\"\n",
    "    Convert RGB mask to single-channel class index mask.\n",
    "    mask: HxWx3 numpy array\n",
    "    color_map: dict mapping (r,g,b) -> class_id\n",
    "    \"\"\"\n",
    "    h, w, _ = mask.shape\n",
    "    class_mask = np.zeros((h, w), dtype=np.int64)\n",
    "    for rgb, idx in color_map.items():\n",
    "        matches = np.all(mask == rgb, axis=-1)\n",
    "        class_mask[matches] = idx\n",
    "    return class_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2961f87",
   "metadata": {},
   "source": [
    "## Geting the Unity Scenes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de137eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarsTerrainDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, use_alt_map=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        def list_images(folder):\n",
    "            valid_exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")\n",
    "            return sorted([f for f in os.listdir(folder) if f.lower().endswith(valid_exts)])\n",
    "\n",
    "        self.images = list_images(img_dir)\n",
    "        self.masks = list_images(mask_dir)\n",
    "\n",
    "\n",
    "\n",
    "        self.color_map = ALT_COLOR_MAP if use_alt_map else COLOR_MAP\n",
    "        assert len(self.images) == len(self.masks), \"Mismatch between images and masks\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = rgb_to_class(mask, self.color_map)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066a5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1600, Validation: 400\n"
     ]
    }
   ],
   "source": [
    "all_images = sorted(os.listdir(IMG_DIR))\n",
    "train_imgs, val_imgs = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "print(f\"Training: {len(train_imgs)}, Validation: {len(val_imgs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4db2c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.RandomCrop(512, 512),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=0.5),\n",
    "    A.Normalize(mean=(0.0,0.0,0.0), std=(1.0,1.0,1.0)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(1024, 1024),\n",
    "    A.Normalize(mean=(0.0,0.0,0.0), std=(1.0,1.0,1.0)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13c4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MarsTerrainDataset(IMG_DIR, MASK_DIR, transform=train_transforms)\n",
    "val_dataset   = MarsTerrainDataset(IMG_DIR, MASK_DIR, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a570646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2d ‚Üí BN ‚Üí ReLU) √ó 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=5, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Down part\n",
    "        for feat in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feat))\n",
    "            in_channels = feat\n",
    "\n",
    "        # Up part (reverse)\n",
    "        for feat in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feat * 2, feat, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feat * 2, feat))\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b92f752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "model = UNet(in_channels=3, out_channels=NUM_CLASSES).to(DEVICE)\n",
    "print(\"Model initialized on:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563cc50",
   "metadata": {},
   "source": [
    "## Defining Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99390fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()              # for multi-class segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    loop = tqdm(loader, leave=False)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device)\n",
    "        targets = targets.long().to(device)\n",
    "\n",
    "        # forward\n",
    "        preds = model(data)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_fn(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            preds = model(data)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7781e8d",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a669cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 1.3169 | Val Loss: 2.7051\n",
      "‚úÖ Validation improved; model saved to best_unet_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50] | Train Loss: 1.2574 | Val Loss: 1.4072\n",
      "‚úÖ Validation improved; model saved to best_unet_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50] | Train Loss: 1.2320 | Val Loss: 2.2951\n",
      "‚ö†Ô∏è No improvement for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50] | Train Loss: 1.2352 | Val Loss: 2.0103\n",
      "‚ö†Ô∏è No improvement for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] | Train Loss: 1.2300 | Val Loss: 1.2563\n",
      "‚úÖ Validation improved; model saved to best_unet_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50] | Train Loss: 1.2126 | Val Loss: 1.5633\n",
      "‚ö†Ô∏è No improvement for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50] | Train Loss: 1.2337 | Val Loss: 1.4939\n",
      "‚ö†Ô∏è No improvement for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50] | Train Loss: 1.2168 | Val Loss: 1.3838\n",
      "‚ö†Ô∏è No improvement for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50] | Train Loss: 1.1980 | Val Loss: 1.5552\n",
      "‚ö†Ô∏è No improvement for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50] | Train Loss: 1.1879 | Val Loss: 1.2752\n",
      "‚ö†Ô∏è No improvement for 5 epochs.\n",
      "‚èπ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model loading\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- logging and checkpoint dirs ---\n",
    "log_dir = \"./runs/unet_training\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience = 5\n",
    "counter = 0\n",
    "num_epochs = 50\n",
    "\n",
    "save_path = \"best_unet_model.pth\"\n",
    "\n",
    "# --- training loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "    for images, masks in loop:\n",
    "        images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # --- validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Val\": val_loss}, epoch)\n",
    "\n",
    "    # --- early stopping ---\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"‚úÖ Validation improved; model saved to {save_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement for {counter} epochs.\")\n",
    "        if counter >= patience:\n",
    "            print(\"‚èπ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56597044",
   "metadata": {},
   "source": [
    "## Now that training is done, we move on to testing on AI4Mars dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4daad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 18127 total images in EDR folder.\n",
      "  ‚Ä¢ 16064 label files in train\n",
      "  ‚Ä¢ 322 label files in masked-gold-min3-100agree\n",
      "‚úÖ Total unique label files: 16386\n",
      "\n",
      "‚úÖ Found 16386 matching image‚Äìlabel pairs.\n",
      "‚úÖ Example matches:\n",
      "   NLB_546279320EDR_F0621530NCAM07753M1.JPG\n",
      "   NLB_499420274EDR_F0501116NCAM00289M1.JPG\n",
      "   NLB_620669790EDR_F0763002NCAM00341M1.JPG\n",
      "   NLB_546462602EDR_F0622026NCAM00260M1.JPG\n",
      "   NLB_509001376EDR_F0522668NCAM07753M1.JPG\n",
      "   NLA_408594521EDR_F0051398NCAM05134M1.JPG\n",
      "   NLB_485828380EDR_F0481530NCAM07753M1.JPG\n",
      "   NLB_468426251EDR_F0441140NCAM00353M1.JPG\n",
      "   NLB_559954945EDR_F0660952NCAM00270M1.JPG\n",
      "   NLB_616601909EDR_F0762194NCAM00259M1.JPG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------\n",
    "IMAGE_DIR = \"/home/uname/Unity-Mars-Simulation/training-and-testing/ai4mars-dataset-merged-0.1/msl/images/edr\"\n",
    "LABEL_DIRS = [\n",
    "    \"/home/uname/Unity-Mars-Simulation/training-and-testing/ai4mars-dataset-merged-0.1/msl/labels/train\",\n",
    "    \"/home/uname/Unity-Mars-Simulation/training-and-testing/ai4mars-dataset-merged-0.1/msl/labels/test/masked-gold-min3-100agree\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# FAST CHECK\n",
    "# ------------------------------------------------\n",
    "image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.png'))]\n",
    "print(f\"‚úÖ Found {len(image_files)} total images in EDR folder.\")\n",
    "\n",
    "# Collect all label filenames from both folders\n",
    "label_files = set()\n",
    "for ld in LABEL_DIRS:\n",
    "    if not os.path.exists(ld):\n",
    "        print(f\"‚ö†Ô∏è Warning: Label directory not found: {ld}\")\n",
    "        continue\n",
    "    files = [f for f in os.listdir(ld) if f.lower().endswith('.png')]\n",
    "    print(f\"  ‚Ä¢ {len(files)} label files in {os.path.basename(ld)}\")\n",
    "    label_files.update(files)\n",
    "\n",
    "print(f\"‚úÖ Total unique label files: {len(label_files)}\")\n",
    "\n",
    "# Match by base name (ignore extension)\n",
    "matches = []\n",
    "for img in image_files:\n",
    "    base = os.path.splitext(img)[0]\n",
    "    for suffix in [\"_merged.png\", \".png\"]:\n",
    "        if base + suffix in label_files:\n",
    "            matches.append(img)\n",
    "            break\n",
    "\n",
    "print(f\"\\n‚úÖ Found {len(matches)} matching image‚Äìlabel pairs.\")\n",
    "if matches:\n",
    "    print(\"‚úÖ Example matches:\")\n",
    "    for m in matches[:10]:\n",
    "        print(\"  \", m)\n",
    "else:\n",
    "    print(\"‚ùå No matches found ‚Äî check naming conventions (e.g. '_merged' suffix).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896827e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 16386 matching image-label pairs out of 18130 images\n",
      "‚ö° Running quick evaluation on 16386 samples only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16386/16386 [1:03:38<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete\n",
      "Samples evaluated: 16386\n",
      "Mean IoU: 0.1002\n",
      "Pixel Accuracy: 0.2333\n",
      "üìÑ Progress saved in: progress_log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------\n",
    "IMAGE_DIR = \"/home/uname/Unity-Mars-Simulation/training-and-testing/ai4mars-dataset-merged-0.1/msl/images/edr\"\n",
    "LABEL_DIRS = [\n",
    "    \"/home/uname/Unity-Mars-Simulation/training-and-testing/ai4mars-dataset-merged-0.1/msl/labels/train\",\n",
    "    \"/home/uname/Unity-Mars-Simulation/training-and-testing/ai4mars-dataset-merged-0.1/msl/labels/test/masked-gold-min3-100agree\",\n",
    "]\n",
    "MODEL_PATH = \"best_unet_model.pth\"\n",
    "NUM_CLASSES = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROGRESS_FILE = \"progress_log.txt\"\n",
    "\n",
    "# ------------------------------------------------\n",
    "# DATASET CLASS\n",
    "# ------------------------------------------------\n",
    "class MarsEvalDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dirs):\n",
    "        self.samples = []\n",
    "        for f in os.listdir(image_dir):\n",
    "            if not f.lower().endswith((\".jpg\", \".png\")):\n",
    "                continue\n",
    "            base = os.path.splitext(f)[0]\n",
    "\n",
    "            mask_path = None\n",
    "            for ld in label_dirs:\n",
    "                for suffix in [\"_merged.png\", \".png\"]:\n",
    "                    candidate = os.path.join(ld, base + suffix)\n",
    "                    if os.path.exists(candidate):\n",
    "                        mask_path = candidate\n",
    "                        break\n",
    "                if mask_path:\n",
    "                    break\n",
    "\n",
    "            if mask_path:\n",
    "                self.samples.append((os.path.join(image_dir, f), mask_path))\n",
    "\n",
    "        print(f\"‚úÖ Found {len(self.samples)} matching image-label pairs out of {len(os.listdir(image_dir))} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.samples[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not read {img_path}\")\n",
    "        if img.ndim == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (1024, 1024))\n",
    "        img_tensor = torch.tensor(img).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        gt = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt = cv2.resize(gt, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "        gt[gt == 255] = 4   # ‚úÖ FIX: map 255 ‚Üí class 4\n",
    "        return img_tensor, torch.tensor(gt, dtype=torch.long), os.path.basename(img_path)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# METRICS\n",
    "# ------------------------------------------------\n",
    "def compute_iou_and_acc(pred, gt, num_classes):\n",
    "    ious = []\n",
    "    acc = (pred == gt).sum() / np.prod(gt.shape)\n",
    "    for c in range(num_classes):\n",
    "        inter = np.logical_and(pred == c, gt == c).sum()\n",
    "        union = np.logical_or(pred == c, gt == c).sum()\n",
    "        if union > 0:\n",
    "            ious.append(inter / union)\n",
    "    return np.mean(ious), acc\n",
    "\n",
    "# ------------------------------------------------\n",
    "# MODEL LOAD\n",
    "# ------------------------------------------------\n",
    "model = UNet(in_channels=3, out_channels=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# DATA LOAD\n",
    "# ------------------------------------------------\n",
    "dataset = MarsEvalDataset(IMAGE_DIR, LABEL_DIRS)\n",
    "\n",
    "# ‚ö° QUICK TEST MODE (20 samples only)\n",
    "# dataset.samples = dataset.samples[:20]\n",
    "print(f\"‚ö° Running quick evaluation on {len(dataset)} samples only.\")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# EVALUATION LOOP\n",
    "# ------------------------------------------------\n",
    "iou_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "if os.path.exists(PROGRESS_FILE):\n",
    "    os.remove(PROGRESS_FILE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (imgs, gts, names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        preds = model(imgs)\n",
    "        preds = torch.argmax(F.softmax(preds, dim=1), dim=1).cpu().numpy()[0]\n",
    "        gt = gts.squeeze().numpy()\n",
    "\n",
    "        iou, acc = compute_iou_and_acc(preds, gt, NUM_CLASSES)\n",
    "        iou_scores.append(iou)\n",
    "        acc_scores.append(acc)\n",
    "\n",
    "        # Save progress every 10 samples ‚úÖ\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            mean_iou = float(np.mean(iou_scores))\n",
    "            mean_acc = float(np.mean(acc_scores))\n",
    "            with open(PROGRESS_FILE, \"a\") as f:\n",
    "                f.write(f\"{idx+1},{mean_iou:.4f},{mean_acc:.4f}\\n\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete\")\n",
    "print(f\"Samples evaluated: {len(iou_scores)}\")\n",
    "if len(iou_scores) > 0:\n",
    "    print(f\"Mean IoU: {np.mean(iou_scores):.4f}\")\n",
    "    print(f\"Pixel Accuracy: {np.mean(acc_scores):.4f}\")\n",
    "    print(f\"üìÑ Progress saved in: {PROGRESS_FILE}\")\n",
    "else:\n",
    "    print(\"‚ùå No matching samples evaluated ‚Äî check folder structure again.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
